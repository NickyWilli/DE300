{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DE300 HW3 - Nicky Williams"
      ],
      "metadata": {
        "id": "8QD02fcz4Nz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. tf-idf definition"
      ],
      "metadata": {
        "id": "1p5_5yTN4lGD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-zLtGrg4IQo",
        "outputId": "c5cfba51-1202-43ba-dfa2-5279f653819e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 33.2M  100 33.2M    0     0  42.6M      0 --:--:-- --:--:-- --:--:-- 42.5M\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv -O"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"AG news\")\n",
        "         .getOrCreate()\n",
        "        )\n",
        "\n",
        "agnews = spark.read.csv(\"agnews_clean.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# turning the second column from a string to an array\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))\n",
        "\n",
        "# each row contains the document id and a list of filtered words\n",
        "agnews.show(5, truncate=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0F_SXSV5Bbm",
        "outputId": "6ca1536b-68bb-42b3-9873-0834d4cc4245"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------+\n",
            "|_c0|                      filtered|\n",
            "+---+------------------------------+\n",
            "|  0|[wall, st, bears, claw, bac...|\n",
            "|  1|[carlyle, looks, toward, co...|\n",
            "|  2|[oil, economy, cloud, stock...|\n",
            "|  3|[iraq, halts, oil, exports,...|\n",
            "|  4|[oil, prices, soar, time, r...|\n",
            "+---+------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "# Adding doc id to each row (got error before doing this)\n",
        "agnews = agnews.withColumn(\"doc_id\",F.monotonically_increasing_id())\n",
        "\n",
        "# Creating pairs of (doc_id, term)\n",
        "doc_term_rdd = agnews.select(\"doc_id\",\"filtered\").rdd.flatMap(\n",
        "    lambda row: [(row[\"doc_id\"], term) for term in row[\"filtered\"]]\n",
        ")\n",
        "\n",
        "\n",
        "# Counting the num of times each term appears in each doc\n",
        "tf_counts = doc_term_rdd.map(lambda x:((x[0], x[1]), 1)) \\\n",
        "                        .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "\n",
        "# Counting total numb of terms in each document\n",
        "doc_lengths = agnews.select(\"doc_id\", F.size(\"filtered\").alias(\"doc_len\")) \\\n",
        "                    .rdd.map(tuple) \\\n",
        "                    .collectAsMap()\n",
        "\n",
        "\n",
        "\n",
        "# Computing normalized term frequency where tf = (# occurrences of term in doc) / (total terms in doc)\n",
        "tf_rdd = tf_counts.map(lambda x:(x[0], x[1] / doc_lengths[x[0][0]]))\n",
        "\n",
        "\n",
        "# Retrieving all unique pairs to find how many docs contain each term\n",
        "term_doc_rdd = doc_term_rdd.distinct().map(lambda x: (x[1], x[0]))\n",
        "\n",
        "\n",
        "\n",
        "# Counting num of distinct docs each term appears in\n",
        "doc_freq = term_doc_rdd.map(lambda x: (x[0], 1)) \\\n",
        "                       .reduceByKey(lambda a,b: a + b)\n",
        "\n",
        "\n",
        "# Total num of docs in the data\n",
        "num_docs = agnews.count()\n",
        "\n",
        "\n",
        "# Computing IDF for each term where idf = log(num_docs / doc_frequency)\n",
        "idf_rdd = doc_freq.map(lambda x:(x[0],math.log(num_docs / x[1])))\n",
        "\n",
        "\n",
        "# Converting to dictionary to make rest of code easier to implement\n",
        "idf_dict = dict(idf_rdd.collect())\n",
        "\n",
        "\n",
        "# Computing TF-IDF by multiplying tf and idf for each (doc_id, term) pair\n",
        "tfidf_rdd = tf_rdd.map(lambda x:(x[0], x[1] * idf_dict.get(x[0][1], 0)))\n"
      ],
      "metadata": {
        "id": "JWQrzDul5Wzz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2\n",
        "\n",
        "# Grouping the TF-IDF scores by doc_id so that each row (doc) gets a list of pairs (term, tf-idf)\n",
        "tfidf_grouped = tfidf_rdd.map(lambda x: (x[0][0],(x[0][1], x[1]))) \\\n",
        "                         .groupByKey() \\\n",
        "                         .mapValues(list)\n",
        "\n",
        "\n",
        "# Converting the grouped rdd into a df with schema (doc_id, tfidf) --> each row will have a doc_id and its corresponding tuples (term, tf-idf)\n",
        "tfidf_df = tfidf_grouped.toDF([\"doc_id\", \"tfidf\"])\n",
        "\n",
        "\n",
        "# Adding a new col called \"tfidf\" to agnews_with_tfidf to join scored back with the og df\n",
        "agnews_with_tfidf = agnews.join(tfidf_df, on=\"doc_id\", how=\"left\")\n"
      ],
      "metadata": {
        "id": "NfoR80Qu8nz2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 3\n",
        "\n",
        "# Print out the tf-idf measure for the first 5 documents\n",
        "agnews_with_tfidf.select(\"doc_id\", \"tfidf\").orderBy(\"doc_id\").show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vDTVWSw9MbB",
        "outputId": "4d8de4ef-188f-4f7d-a56d-886a0a6a575e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|doc_id|tfidf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0     |[{claw, 0.499114829314058}, {back, 0.1892216338539946}, {black, 0.2953171727366614}, {reuters, 0.24754017186645658}, {short, 0.2773120373951269}, {band, 0.3643421454792778}, {ultra, 0.4125512394225831}, {green, 0.2877107940095433}, {wall, 0.5115985326511431}, {st, 0.2584728642725166}, {bears, 0.3372044607529448}, {sellers, 0.4468379768438066}, {street, 0.24678348986493034}, {dwindling, 0.4572386180709258}, {cynics, 0.563734318747707}, {seeing, 0.37743394553516213}]                                                                                                                                                                                                                                                                                                                            |\n",
            "|1     |[{looks, 0.1973537176743789}, {commercial, 0.2057832028092643}, {private, 0.1929050573011279}, {investment, 0.1890771769001148}, {firm, 0.15969712503706046}, {reputation, 0.2578098186776328}, {plays, 0.22418048797172685}, {quietly, 0.25188254045524316}, {bets, 0.27861293130724324}, {market, 0.13394932212703356}, {carlyle, 0.7168306746824437}, {toward, 0.1898997183872362}, {aerospace, 0.2581171817448437}, {reuters, 0.1650267812443044}, {group, 0.12468100563149095}, {making, 0.1698717076460444}, {well, 0.17053284421704767}, {timed, 0.324478643568105}, {occasionally, 0.33274321954270536}, {controversial, 0.20949395177306526}, {defense, 0.1751279339938823}, {industry, 0.15043731768548949}, {placed, 0.2284965552404658}, {another, 0.14507889141437585}, {part, 0.16022031730914288}]|\n",
            "|2     |[{oil, 0.13908157105107033}, {cloud, 0.295159450642955}, {stocks, 0.14976769101715193}, {outlook, 0.4265073217271922}, {reuters, 0.18565512889984243}, {plus, 0.24449073714833106}, {expected, 0.16094627131903613}, {hang, 0.30475018305843793}, {week, 0.13121900794126834}, {summer, 0.22694739048609625}, {doldrums, 0.3770252270329423}, {economy, 0.3721400726458204}, {soaring, 0.2596334462817101}, {crude, 0.197241148492091}, {prices, 0.14472559202114177}, {worries, 0.23009353850726894}, {earnings, 0.1792714404894228}, {stock, 0.17879168082328206}, {market, 0.15069298739291276}, {next, 0.14062721303262238}, {depth, 0.31343954772064864}]                                                                                                                                                   |\n",
            "|3     |[{iraq, 0.23809526243476142}, {halts, 0.27365396741681164}, {southern, 0.336553609483104}, {pipeline, 0.4720829409342409}, {authorities, 0.18159366801541998}, {export, 0.23862435123782139}, {intelligence, 0.20782569445751425}, {infrastructure, 0.22959926718225876}, {official, 0.15149485319300557}, {said, 0.06593367258642661}, {saturday, 0.12197305137253434}, {oil, 0.35763832555989516}, {exports, 0.2146590164054526}, {main, 0.36492623402353547}, {reuters, 0.15913296762843637}, {halted, 0.2557691357056513}, {flows, 0.2774168429760197}, {showed, 0.1743365558077232}, {rebel, 0.18209445014364567}, {militia, 0.2252006141545402}, {strike, 0.17411586950893898}]                                                                                                                            |\n",
            "|4     |[{oil, 0.22253051368171256}, {soar, 0.2306791247647116}, {new, 0.1271397626254836}, {menace, 0.5747440955975784}, {us, 0.1669859687392097}, {world, 0.09332201126546583}, {toppling, 0.27964532733021175}, {straining, 0.2904044404056468}, {present, 0.22209684830286883}, {barely, 0.21935019724396657}, {three, 0.10314988960754677}, {months, 0.14002501854271598}, {presidential, 0.1480257381794347}, {prices, 0.23156094723382684}, {time, 0.10623532598945136}, {record, 0.1232987151692413}, {posing, 0.2589223867776184}, {economy, 0.14885602905832815}, {afp, 0.2559170042376607}, {tearaway, 0.3918885216630942}, {records, 0.19759033440942064}, {wallets, 0.2665151844733088}, {economic, 0.14782686453681568}, {elections, 0.16009904796740967}]                                                 |\n",
            "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. SVM objective function"
      ],
      "metadata": {
        "id": "sfU966bj-t9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv -O\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMUSY99Q-w00",
        "outputId": "43c73414-7fc7-440d-bf1a-a91b9afd1b81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1391  100  1391    0     0   4504      0 --:--:-- --:--:-- --:--:--  4516\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    22  100    22    0     0     54      0 --:--:-- --:--:-- --:--:--    55\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 61.9M  100 61.9M    0     0  25.8M      0  0:00:02  0:00:02 --:--:-- 25.8M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1/2/3\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Building spark session\n",
        "spark = SparkSession.builder.appName(\"SVM Loss\").getOrCreate()\n",
        "\n",
        "\n",
        "# Loading data into X and y rdds\n",
        "data_df = spark.read.csv(\"data_for_svm.csv\", inferSchema=True, header=False)\n",
        "X_rdd = data_df.rdd.map(lambda row: np.array(row[:-1]))  # features (64-dim)\n",
        "y_rdd = data_df.rdd.map(lambda row: row[-1]) # labels (either +1 or -1)\n",
        "\n",
        "# Loading w and b\n",
        "w = np.array(pd.read_csv(\"w.csv\", header=None)).flatten()\n",
        "b = float(pd.read_csv(\"bias.csv\", header=None).values[0][0])\n",
        "\n",
        "\n",
        "\n",
        "# TASK 2 - Defining  SVM loss function using MapReduce functions\n",
        "def loss_SVM(w, b, X_rdd, y_rdd, lam=0.01):\n",
        "\n",
        "    data_rdd = X_rdd.zip(y_rdd)  #combine to (xi, yi)\n",
        "    n = data_rdd.count()\n",
        "    if n == 0:\n",
        "        return None\n",
        "\n",
        "\n",
        "    # TASK 1 - Designing MapReduce functions\n",
        "    # Computing hinge losses\n",
        "    hinge_losses = data_rdd.map(lambda row: max(0, 1 - row[1] * (np.dot(w, row[0]) +b)))\n",
        "    # Summing hinge losses\n",
        "    total_hinge_loss = hinge_losses.reduce(lambda a,b: a + b)\n",
        "\n",
        "\n",
        "\n",
        "    # Computing regularization term\n",
        "    reg_term = lam * np.linalg.norm(w) ** 2\n",
        "\n",
        "    # Returning SVM objective value\n",
        "    return reg_term + (total_hinge_loss/ n)\n",
        "\n",
        "\n",
        "# TASK 3 - predicting for all of the data\n",
        "# Running and printing the loss\n",
        "svm_loss_value = loss_SVM(w, b, X_rdd, y_rdd)\n",
        "print(f\"Loss from loss_SVM(): {svm_loss_value:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS-2_FttCL0i",
        "outputId": "a6ad91c0-f793-4545-c0bb-05682c54ba53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss from loss_SVM(): 0.999775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 4\n",
        "\n",
        "# Loading data\n",
        "data_df = spark.read.csv(\"data_for_svm.csv\", inferSchema=True, header=False)\n",
        "X_rdd = data_df.rdd.map(lambda row: np.array(row[:-1])) #features\n",
        "\n",
        "# Loading weights and bias\n",
        "w = np.array(pd.read_csv(\"w.csv\", header=None)).flatten()\n",
        "b = float(pd.read_csv(\"bias.csv\", header=None).values[0][0])\n",
        "\n",
        "# Defining MapReduce func\n",
        "def predict_svm(w, b, X_rdd):\n",
        "\n",
        "    return X_rdd.map(lambda x: 1 if np.dot(w, x) + b >= 0 else -1)\n",
        "\n",
        "# Predictions\n",
        "predictions_rdd = predict_svm(w, b, X_rdd)\n",
        "\n",
        "# Printing first 10 predictions\n",
        "print(\"First 10 predictions:\")\n",
        "print(predictions_rdd.take(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfXZj8ifECOM",
        "outputId": "24075451-0bed-4fd9-a09b-01c162f1284c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 predictions:\n",
            "[-1, -1, -1, 1, -1, 1, -1, -1, 1, -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GenAI statement:** I worked on HW in large part with TAs who would help me use ChatGPT when I came across errors. I had issues defining the SVM loss function in part 2 and chat told me to add:  \n",
        "\n",
        "\"n = data_rdd.count()\n",
        "    if n == 0:\n",
        "        print(\"Dataset is empty\")\n",
        "        return None\"\n",
        "\n",
        "\n",
        "I also had issues loading w and b because I didnt add .flatten() for w and didn't add the float or .values[0][0] for b:\n",
        "\n",
        "w = np.array(pd.read_csv(\"w.csv\", header=None)).flatten()\n",
        "\n",
        "b = float(pd.read_csv(\"bias.csv\", header=None).values[0][0])\n",
        "\n",
        "\n",
        "Other help was from TAs and their GenAi sources\n"
      ],
      "metadata": {
        "id": "HxSu4C63SFk5"
      }
    }
  ]
}